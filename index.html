<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Zhuofeng Li </title> <meta name="author" content="Zhuofeng Li"> <meta name="description" content="Zhuofeng Li's academic homepage. CS PhD student at Texas A&amp;M University researching LLM/VLM post-training, reinforcement learning, alignment, and evaluation. Seeking research collaborations and internship opportunities in AI and machine learning. "> <meta name="keywords" content="machine learningAIartificial intelligenceresearchLLMVLMpost-trainingreinforcement learningalignmentevaluationcomputer scienceZhuofeng Li"> <meta property="og:site_name" content="Zhuofeng Li"> <meta property="og:type" content="website"> <meta property="og:title" content="Zhuofeng Li | about"> <meta property="og:url" content="https://Zhuofeng-Li.github.io/"> <meta property="og:description" content="Zhuofeng Li's academic homepage. CS PhD student at Texas A&amp;M University researching LLM/VLM post-training, reinforcement learning, alignment, and evaluation. Seeking research collaborations and internship opportunities in AI and machine learning. "> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="about"> <meta name="twitter:description" content="Zhuofeng Li's academic homepage. CS PhD student at Texas A&amp;M University researching LLM/VLM post-training, reinforcement learning, alignment, and evaluation. Seeking research collaborations and internship opportunities in AI and machine learning. "> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Zhuofeng Li"
        },
        "url": "https://Zhuofeng-Li.github.io/",
        "@type": "WebSite",
        "description": "Zhuofeng Li's academic homepage. CS PhD student at Texas A&M University researching LLM/VLM post-training, reinforcement learning, alignment, and evaluation. Seeking research collaborations and internship opportunities in AI and machine learning.
",
        "headline": "about",
        
        "name": "Zhuofeng Li",
        "@context": "https://schema.org"
    }
  </script> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/prof_pic.jpg?377cae67aaf1bef232610783e9a7201c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zhuofeng-li.github.io/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Zhuofeng</span> Li </h1> <p class="desc"><a href="https://www.tamu.edu/index.html" rel="external nofollow noopener" target="_blank">Texas A&amp;M University</a>; <a href="https://www.stanford.edu/" rel="external nofollow noopener" target="_blank">Stanford University</a>;</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic.jpg?377cae67aaf1bef232610783e9a7201c" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>Hi there 👋. My name is Zhuofeng Li. I’m a CS Ph.D. student at <a href="https://www.tamu.edu/index.html" rel="external nofollow noopener" target="_blank">Texas A&amp;M University</a>, advised by Prof. <a href="https://yuzhimanhua.github.io/" rel="external nofollow noopener" target="_blank">Yu Zhang</a>. I am also a visiting student at <a href="https://www.stanford.edu/" rel="external nofollow noopener" target="_blank">Stanford University</a> working with Prof. <a href="https://yejinc.github.io/" rel="external nofollow noopener" target="_blank">Yejin Choi</a> and Prof. <a href="https://www.james-zou.com/" rel="external nofollow noopener" target="_blank">James Zou</a>. Previously, I interned <a href="https://github.com/TIGER-AI-Lab" rel="external nofollow noopener" target="_blank">TIGER-Lab</a> at University of Waterloo working with Prof. <a href="https://wenhuchen.github.io/" rel="external nofollow noopener" target="_blank">Wenhu Chen</a>.</p> <p>My research lies in LLM/VLM-post training, including reasoning, alignment, evaluation, and applications. Recently, I am particularly focus on agentic reinforcement learning. As part of this direction, I as the core contributor build <a href="https://github.com/TIGER-AI-Lab/verl-tool" rel="external nofollow noopener" target="_blank">VerlTool</a> (500+ stars), a unified and extensible framework for tool-agent RL training.</p> <p>I am actively seeking research collaborations and intern opportunities in LLM/VLM post-training, reinforcement learning, agents, and other exciting directions.</p> <p><strong>Feel free to reach out me through zhuofengli12345@gmail.com</strong></p> <div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; margin: 20px 0; box-shadow: 0 4px 6px rgba(0,0,0,0.1);"> <p style="color: white; margin: 0; font-size: 1.1em;"> 🚨 <strong>Paper Alert!</strong><br> Thrilled to introduce <strong>AgentFlow</strong> — a <strong>trainable</strong>, <strong>tool-integrated agentic framework</strong> that <strong>directly optimizes agents within the system in an online fashion</strong> using <strong>Flow-GRPO</strong> 🌀💫, achieving <strong>superior tool use</strong> 🛠 and <strong>long-horizon reasoning</strong> 🧠 across diverse domains. <br><br> 📄 <a href="https://arxiv.org/abs/2510.05592" style="color: #ffd700;" rel="external nofollow noopener" target="_blank">Paper</a> | 💻 <a href="https://github.com/lupantech/AgentFlow" style="color: #ffd700;" rel="external nofollow noopener" target="_blank">Code</a> | 🌐 <a href="https://agentflow.stanford.edu" style="color: #ffd700;" rel="external nofollow noopener" target="_blank">Website</a> </p> </div> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Oct 02, 2025</th> <td> 🚀 Excited to share <a href="https://tiger-ai-lab.github.io/VideoScore2/" rel="external nofollow noopener" target="_blank">VideoScore2</a> — bringing RL to generative video evaluation with scoring and rich reasoning traces! </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 22, 2025</th> <td> 🎉 <a href="https://github.com/TIGER-AI-Lab/verl-tool" rel="external nofollow noopener" target="_blank">VerlTool</a> is approaching 550+ stars on GitHub! It’s becoming a key framework for Tool-Agent RL training. </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 18, 2025</th> <td> 🎉 VerlTool’s tech report (co-first author) is out! Please see on <a href="https://huggingface.co/papers/2509.01055" rel="external nofollow noopener" target="_blank">Hugging Face Daily Paper</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 15, 2025</th> <td> 🚀 Starting my CS PhD Journey at <a href="https://www.tamu.edu/index.html" rel="external nofollow noopener" target="_blank">Teaxs A&amp;M University</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Aug 13, 2025</th> <td> 🤝 Our paper GReF (co-first author) has been accepted to CIKM 2026. This recommendation reranking LLM project was conducted during my internship at Kuaishou. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <p style="font-size: 0.9em; margin-bottom: 1em; color: #666;">*Co-first Author</p> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Arxiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/agentflow.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="agentflow.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2025agentflow" class="col-sm-8"> <div class="title">In-The-Flow Agentic System Optimization for Effective Planning and Tool Use</div> <div class="author"> <em>Zhuofeng Li</em> <sup>*</sup> , Haoxiang Zhang <sup>*</sup> , Seungju Han , and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Sheng Liu, Jianwen Xie, Yu Zhang, Yejin Choi, James Zou, Pan Lu' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>In arxiv preprint</em>, Oct 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2510.05592" rel="external nofollow noopener" target="_blank"> <img alt="arXiv" src="https://img.shields.io/badge/arXiv-b31b1b.svg?logo=arXiv"> </a> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/-Abs-FFCC99?style=flat"></a> <a class="bibtex z-depth-0" role="button"><img alt="Bibtex" src="https://img.shields.io/badge/-Bib-FFCC99?style=flat"></a> <a href="https://agentflow.stanford.edu/#" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-FFCC99?style=flat"> </a> <a href="https://huggingface.co/AgentFlow" rel="external nofollow noopener" target="_blank"> <img alt="Huggingface Collections" src="https://img.shields.io/badge/-%F0%9F%A4%97%20Huggingface-red?style=flat"> </a> <a href="https://github.com/https://github.com/lupantech/AgentFlow" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/https://github.com/lupantech/AgentFlow"></a> </div> <div class="abstract hidden"> <p>Outcome-driven reinforcement learning has advanced reasoning in large language models (LLMs), but prevailing tool-augmented approaches train a single, monolithic policy that interleaves thoughts and tool calls under full context; this scales poorly with long horizons and diverse tools and generalizes weakly to new scenarios. Agentic systems offer a promising alternative by decomposing work across specialized modules, yet most remain training-free or rely on offline training decoupled from the live dynamics of multi-turn interaction. We introduce AgentFlow, a trainable, in-the-flow agentic framework that coordinates four modules (planner, executor, verifier, generator) through an evolving memory and directly optimizes its planner inside the multi-turn loop. To train on-policy in live environments, we propose Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles long-horizon, sparse-reward credit assignment by converting multi-turn optimization into a sequence of tractable single-turn policy updates. It broadcasts a single, verifiable trajectory-level outcome to every turn to align local planner decisions with global success and stabilizes learning with group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale backbone outperforms top-performing baselines with average accuracy gains of 14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on scientific tasks, even surpassing larger proprietary models like GPT-4o. Further analyses confirm the benefits of in-the-flow optimization, showing improved planning, enhanced tool-calling reliability, and positive scaling with model size and reasoning turns.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2025agentflow</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{In-The-Flow Agentic System Optimization for Effective Planning and Tool Use}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Zhuofeng and Zhang, Haoxiang and Han, Seungju and Liu, Sheng and Xie, Jianwen and Zhang, Yu and Choi, Yejin and Zou, James and Lu, Pan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{arxiv preprint}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">github</span> <span class="p">=</span> <span class="s">{https://github.com/lupantech/AgentFlow}</span><span class="p">,</span>
  <span class="na">huggingface</span> <span class="p">=</span> <span class="s">{https://huggingface.co/AgentFlow}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">num_co_first_author</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Arxiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/verltool.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="verltool.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jiang2025verltool" class="col-sm-8"> <div class="title">VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use</div> <div class="author"> Dongfu Jiang <sup>*</sup> , Yi Lu <sup>*</sup> , <em>Zhuofeng Li</em> <sup>*</sup> , and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, Tianyu Pang, Wenhu Chen' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </div> <div class="periodical"> <em>In arxiv preprint</em>, Sep 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2509.01055" rel="external nofollow noopener" target="_blank"> <img alt="arXiv" src="https://img.shields.io/badge/arXiv-b31b1b.svg?logo=arXiv"> </a> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/-Abs-FFCC99?style=flat"></a> <a class="bibtex z-depth-0" role="button"><img alt="Bibtex" src="https://img.shields.io/badge/-Bib-FFCC99?style=flat"></a> <a href="https://huggingface.co/papers/2509.01055" rel="external nofollow noopener" target="_blank"> <img alt="Huggingface Collections" src="https://img.shields.io/badge/-%F0%9F%A4%97%20Huggingface-red?style=flat"> </a> <a href="https://x.com/zhuofengli96475/status/1963216180438814901" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/-Tweet-1DA1F2?style=flat&amp;logo=x"></a> <a href="https://github.com/TIGER-AI-Lab/verl-tool" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/TIGER-AI-Lab/verl-tool"></a> </div> <div class="abstract hidden"> <p>Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multi-turn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VerlTool, a unified and modular framework that addresses these limitations through systematic design principles. VerlTool provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2× speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multi-turn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jiang2025verltool</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiang, Dongfu and Lu, Yi and Li, Zhuofeng and Lyu, Zhiheng and Nie, Ping and Wang, Haozhe and Su, Alex and Chen, Hui and Zou, Kai and Du, Chao and Pang, Tianyu and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{arxiv preprint}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">github</span> <span class="p">=</span> <span class="s">{TIGER-AI-Lab/verl-tool}</span><span class="p">,</span>
  <span class="na">huggingface</span> <span class="p">=</span> <span class="s">{https://huggingface.co/papers/2509.01055}</span><span class="p">,</span>
  <span class="na">twitter</span> <span class="p">=</span> <span class="s">{https://x.com/zhuofengli96475/status/1963216180438814901}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">num_co_first_author</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CIKM 2025</abbr> <figure> <picture> <img src="/assets/img/publication_preview/gref.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="gref.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lin2025gref" class="col-sm-8"> <div class="title">GReF: A Unified Generative Framework for Efficient Reranking via Ordered Multi-token Prediction</div> <div class="author"> Zhijie Lin <sup>*</sup> , <em>Zhuofeng Li</em> <sup>*</sup> , Chenglei Dai , and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Wentian Bao, Shuai Lin, Enyun Yu, Haoxiang Zhang, Liang Zhao' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 34th ACM International Conference on Information and Knowledge Management</em>, Nov 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/-Abs-FFCC99?style=flat"></a> <a class="bibtex z-depth-0" role="button"><img alt="Bibtex" src="https://img.shields.io/badge/-Bib-FFCC99?style=flat"></a> </div> <div class="abstract hidden"> <p>In a multi-stage recommendation system, reranking plays a crucial role in modeling intra-list correlations among items. A key challenge lies in exploring optimal sequences within the combinatorial space of permutations. Recent research follows a two-stage (generator-evaluator) paradigm, where a generator produces multiple feasible sequences, and an evaluator selects the best one. In practice, the generator is typically implemented as an autoregressive model. However, these two-stage methods face two main challenges. First, the separation of the generator and evaluator hinders end-to-end training. Second, autoregressive generators suffer from inference efficiency. In this work, we propose a Unified Generative Efficient Reranking Framework (GReF) to address the two primary challenges. Specifically, we introduce Gen-Reranker, an autoregressive generator featuring a bidirectional encoder and a dynamic autoregressive decoder to generate causal reranking sequences. Subsequently, we pre-train Gen-Reranker on the item exposure order for high-quality parameter initialization. To eliminate the need for the evaluator while integrating sequence-level evaluation during training for end-to-end optimization, we propose post-training the model through Rerank-DPO. Moreover, for efficient autoregressive inference, we introduce ordered multi-token prediction (OMTP), which trains Gen-Reranker to simultaneously generate multiple future items while preserving their order, ensuring practical deployment in real-time recommender systems. Extensive offline experiments demonstrate that GReF outperforms state-of-the-art reranking methods while achieving latency that is nearly comparable to non-autoregressive models. Additionally, GReF has also been deployed in a real-world video app Kuaishou with over 300 million daily active users, significantly improving online recommendation quality.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lin2025gref</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GReF: A Unified Generative Framework for Efficient Reranking via Ordered Multi-token Prediction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lin, Zhijie and Li, Zhuofeng and Dai, Chenglei and Bao, Wentian and Lin, Shuai and Yu, Enyun and Zhang, Haoxiang and Zhao, Liang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 34th ACM International Conference on Information and Knowledge Management}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">num_co_first_author</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">TMLR 2025</abbr> <figure> <picture> <img src="/assets/img/publication_preview/goat.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="goat.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="zhang2025avoiding" class="col-sm-8"> <div class="title">Avoiding Structural Pitfalls: Self-Supervised Low-Rank Feature Tuning for Graph Test-Time Adaptation</div> <div class="author"> Haoxiang Zhang <sup>*</sup> , <em>Zhuofeng Li</em> <sup>*</sup> , Qiannan Zhang , and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Ziyi Kou, Juncheng Li, Shichao Pei' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> <em>In Transactions on Machine Learning Research (TMLR), Oct 2025</em>, Oct 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/-Abs-FFCC99?style=flat"></a> <a class="bibtex z-depth-0" role="button"><img alt="Bibtex" src="https://img.shields.io/badge/-Bib-FFCC99?style=flat"></a> </div> <div class="abstract hidden"> <p>Graph Neural Networks (GNNs) have shown impressive performance on graph-structured data, but they often struggle to generalize when facing distribution shifts at test time. Existing test-time adaptation methods for graphs primarily focus on node feature adjustments while overlooking structural information, leading to suboptimal adaptation. We propose a novel self-supervised low-rank feature tuning approach that jointly considers both node features and graph structure during test-time adaptation. Our method leverages self-supervised learning objectives and low-rank matrix factorization to efficiently adapt GNNs to new graph distributions without requiring labeled data, avoiding common structural pitfalls that plague existing approaches.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">zhang2025avoiding</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Avoiding Structural Pitfalls: Self-Supervised Low-Rank Feature Tuning for Graph Test-Time Adaptation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Haoxiang and Li, Zhuofeng and Zhang, Qiannan and Kou, Ziyi and Li, Juncheng and Pei, Shichao}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research (TMLR), Oct 2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">num_co_first_author</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Arxiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/video_eval_pro.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="video_eval_pro.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ma2025videoeval" class="col-sm-8"> <div class="title">VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation</div> <div class="author"> Wentao Ma , Weiming Ren , Yiming Jia , and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Zhuofeng Li, Ping Nie, Ge Zhang, Wenhu Chen' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In arxiv preprint</em>, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2505.14640" rel="external nofollow noopener" target="_blank"> <img alt="arXiv" src="https://img.shields.io/badge/arXiv-b31b1b.svg?logo=arXiv"> </a> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/-Abs-FFCC99?style=flat"></a> <a class="bibtex z-depth-0" role="button"><img alt="Bibtex" src="https://img.shields.io/badge/-Bib-FFCC99?style=flat"></a> <a href="https://tiger-ai-lab.github.io/VideoEval-Pro/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-FFCC99?style=flat"> </a> <a href="https://huggingface.co/papers/2505.14640" rel="external nofollow noopener" target="_blank"> <img alt="Huggingface Collections" src="https://img.shields.io/badge/-%F0%9F%A4%97%20Huggingface-red?style=flat"> </a> <a href="https://x.com/zhuofengli96475/status/1963216180438814901" rel="external nofollow noopener" target="_blank"><img alt="X (formerly Twitter) URL" src="https://img.shields.io/badge/-Tweet-1DA1F2?style=flat&amp;logo=x"></a> <a href="https://github.com/TIGER-AI-Lab/VideoEval-Pro" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/TIGER-AI-Lab/VideoEval-Pro"></a> </div> <div class="abstract hidden"> <p>Large multimodal models (LMMs) have recently emerged as a powerful tool for long video understanding (LVU), but existing benchmarks have significant limitations. Most current benchmarks rely heavily on multiple-choice questions (MCQs) with inflated evaluation results due to guessing, and many questions have strong priors that allow models to answer without watching the entire video. We propose VideoEval-Pro, a new benchmark with open-ended short-answer questions that assess segment-level and full-video understanding through perception and reasoning tasks. By evaluating 21 video LMMs, we discovered significant performance drops (&gt;25\%) on open-ended questions compared to MCQs, higher MCQ scores do not correlate with higher open-ended scores, and VideoEval-Pro benefits more from increased input frames compared to other benchmarks. Our goal is to provide a more realistic and reliable measure of long video understanding.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ma2025videoeval</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ma, Wentao and Ren, Weiming and Jia, Yiming and Li, Zhuofeng and Nie, Ping and Zhang, Ge and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{arxiv preprint}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">github</span> <span class="p">=</span> <span class="s">{TIGER-AI-Lab/VideoEval-Pro}</span><span class="p">,</span>
  <span class="na">huggingface</span> <span class="p">=</span> <span class="s">{https://huggingface.co/papers/2505.14640}</span><span class="p">,</span>
  <span class="na">twitter</span> <span class="p">=</span> <span class="s">{https://x.com/zhuofengli96475/status/1963216180438814901}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Arxiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/structeval.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="structeval.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yang2025structeval" class="col-sm-8"> <div class="title">StructEval: Benchmarking LLMs’ Capabilities to Generate Structural Outputs</div> <div class="author"> Jialin Yang , Dongfu Jiang , Lipeng He , and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Sherman Siu, Yuxuan Zhang, Disen Liao, Zhuofeng Li, Huaye Zeng, Yiming Jia, Haozhe Wang, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>In arxiv preprint</em>, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2505.20139" rel="external nofollow noopener" target="_blank"> <img alt="arXiv" src="https://img.shields.io/badge/arXiv-b31b1b.svg?logo=arXiv"> </a> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/-Abs-FFCC99?style=flat"></a> <a class="bibtex z-depth-0" role="button"><img alt="Bibtex" src="https://img.shields.io/badge/-Bib-FFCC99?style=flat"></a> <a href="https://tiger-ai-lab.github.io/StructEval/" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-FFCC99?style=flat"> </a> <a href="https://huggingface.co/papers/2505.20139" rel="external nofollow noopener" target="_blank"> <img alt="Huggingface Collections" src="https://img.shields.io/badge/-%F0%9F%A4%97%20Huggingface-red?style=flat"> </a> <a href="https://github.com/https://github.com/TIGER-AI-Lab/StructEval" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/https://github.com/TIGER-AI-Lab/StructEval"></a> </div> <div class="abstract hidden"> <p>As Large Language Models (LLMs) become integral to software development workflows, their ability to generate structured outputs has become critically important. We introduce StructEval, a comprehensive benchmark for evaluating LLMs’ capabilities in producing both non-renderable (JSON, YAML, CSV) and renderable (HTML, React, SVG) structured formats. Unlike prior benchmarks, StructEval systematically evaluates structural fidelity across diverse formats through two paradigms: generation tasks and conversion tasks. Our benchmark encompasses 18 formats and 44 types of task, with novel metrics for format adherence and structural correctness. Results reveal significant performance gaps, even state-of-the-art models like o1-mini achieve only 75.58 average score.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yang2025structeval</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Jialin and Jiang, Dongfu and He, Lipeng and Siu, Sherman and Zhang, Yuxuan and Liao, Disen and Li, Zhuofeng and Zeng, Huaye and Jia, Yiming and Wang, Haozhe and others}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{arxiv preprint}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">github</span> <span class="p">=</span> <span class="s">{https://github.com/TIGER-AI-Lab/StructEval}</span><span class="p">,</span>
  <span class="na">huggingface</span> <span class="p">=</span> <span class="s">{https://huggingface.co/papers/2505.20139}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS 2024</abbr> <figure> <picture> <img src="/assets/img/publication_preview/tegdb.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tegdb.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2024teg" class="col-sm-8"> <div class="title">Teg-db: A comprehensive dataset and benchmark of textual-edge graphs</div> <div class="author"> <em>Zhuofeng Li</em> , Zixing Gou , Xiangnan Zhang , and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Zhongyuan Liu, Sirui Li, Yuntong Hu, Chen Ling, Zheng Zhang, Liang Zhao' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, Dec 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2406.10310" rel="external nofollow noopener" target="_blank"> <img alt="arXiv" src="https://img.shields.io/badge/arXiv-b31b1b.svg?logo=arXiv"> </a> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/-Abs-FFCC99?style=flat"></a> <a class="bibtex z-depth-0" role="button"><img alt="Bibtex" src="https://img.shields.io/badge/-Bib-FFCC99?style=flat"></a> <a href="https://github.com/Zhuofeng-Li/TEG-Benchmark" rel="external nofollow noopener" target="_blank"> <img alt="Website" src="https://img.shields.io/badge/-Website-FFCC99?style=flat"> </a> <a href="https://huggingface.co/papers/2406.10310" rel="external nofollow noopener" target="_blank"> <img alt="Huggingface Collections" src="https://img.shields.io/badge/-%F0%9F%A4%97%20Huggingface-red?style=flat"> </a> <a href="https://github.com/Zhuofeng-Li/TEG-Benchmark" rel="external nofollow noopener" target="_blank"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Zhuofeng-Li/TEG-Benchmark"></a> </div> <div class="abstract hidden"> <p>Text-Attributed Graphs (TAGs) augment graph structures with natural language descriptions, but existing datasets primarily feature textual information only at nodes. We introduce TEG-DB, a comprehensive benchmark of textual-edge datasets with rich descriptions on both nodes and edges, spanning domains like citation and social networks. We conducted benchmark experiments to evaluate how current techniques including pre-trained language models and graph neural networks can utilize textual node and edge information. Our goal is to advance research in textual-edge graph analysis and provide deeper insights into complex real-world networks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2024teg</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Teg-db: A comprehensive dataset and benchmark of textual-edge graphs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Zhuofeng and Gou, Zixing and Zhang, Xiangnan and Liu, Zhongyuan and Li, Sirui and Hu, Yuntong and Ling, Chen and Zhang, Zheng and Zhao, Liang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{37}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{60980--60998}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">github</span> <span class="p">=</span> <span class="s">{Zhuofeng-Li/TEG-Benchmark}</span><span class="p">,</span>
  <span class="na">huggingface</span> <span class="p">=</span> <span class="s">{https://huggingface.co/papers/2406.10310}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CIKM 2024</abbr> <figure> <picture> <img src="/assets/img/publication_preview/cfkgc.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cfkgc.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2024learning" class="col-sm-8"> <div class="title">Learning from novel knowledge: Continual few-shot knowledge graph completion</div> <div class="author"> <em>Zhuofeng Li</em> , Haoxiang Zhang , Qiannan Zhang , and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Ziyi Kou, Shichao Pei' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract z-depth-0" role="button"><img alt="Abstract" src="https://img.shields.io/badge/-Abs-FFCC99?style=flat"></a> <a class="bibtex z-depth-0" role="button"><img alt="Bibtex" src="https://img.shields.io/badge/-Bib-FFCC99?style=flat"></a> </div> <div class="abstract hidden"> <p>Knowledge graph completion has been increasingly recognized as a vital approach for uncovering missing knowledge and addressing the incompleteness issue in KGs. To enhance inference on rare relations and mitigate the impact of the long-tail distribution, the dominant strategy designs few-shot models following the meta-learning paradigm. However, these approaches typically operate under the assumption that KGs are available instantly, disregarding the newly emerging relations during KG enrichment. We propose a novel framework designed to equip the few-shot model with the ability to learn sequentially from novel relations through data-level rehearsal and model-level modulation to address catastrophic forgetting, alongside multi-view relation augmentation aimed at resolving the issue of insufficient novel relations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2024learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning from novel knowledge: Continual few-shot knowledge graph completion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Zhuofeng and Zhang, Haoxiang and Zhang, Qiannan and Kou, Ziyi and Pei, Shichao}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 33rd ACM International Conference on Information and Knowledge Management}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1326--1335}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%7A%68%75%6F%66%65%6E%67%6C%69%31%32%33%34%35@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/Zhuofeng-Li" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/zhuofeng-li-6a528626a" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="/feed.xml" title="RSS Feed"><i class="fa-solid fa-square-rss"></i></a> <a href="https://scholar.google.com/citations?user=V3ZBIT4AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a id="WeChatBtn" title="WeChat"><i class="fa-brands fa-weixin"></i></a> <div id="WeChatMod" class="wechat-modal"> <img src="/assets/img/wechat_qr.jpg" alt="WeChat QR" id="WeChatQR"> </div> <script defer src="/assets/js/wechat.js?9f2dc256d1094f6f7ffeade26e50cb50" type="text/javascript"></script> <a href="https://twitter.com/zhuofengli96475" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note">Email and Wechat are the preferred ways to reach me. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zhuofeng Li. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-Y05BDKR167"></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'G-Y05BDKR167');
  </script> <script defer src="/assets/js/google-analytics-setup.js"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>