<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Zhuofeng Li </title> <meta name="author" content="Zhuofeng Li"> <meta name="description" content="publications by categories in reversed chronological order. generated by jekyll-scholar."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/prof_pic.jpg?377cae67aaf1bef232610783e9a7201c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zhuofeng-li.github.io/publications/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Zhuofeng</span> Li </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description">publications by categories in reversed chronological order. generated by jekyll-scholar.</p> </header> <article> <script src="/assets/js/bibsearch.js?1bc438ca9037884cc579601c09afd847" type="module"></script> <p><input type="text" id="bibsearch" spellcheck="false" autocomplete="off" class="search bibsearch-form-input" placeholder="Type to filter"></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Arxiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/verltool.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="verltool.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="jiang2025verltool" class="col-sm-8"> <div class="title">VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use</div> <div class="author"> Dongfu Jiang, Yi Lu, <em>Zhuofeng Li</em>, and <span class="more-authors" title="click to view 9 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '9 more authors' ? 'Zhiheng Lyu, Ping Nie, Haozhe Wang, Alex Su, Hui Chen, Kai Zou, Chao Du, Tianyu Pang, Wenhu Chen' : '9 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">9 more authors</span> </div> <div class="periodical"> <em>In arxiv preprint</em>, Sep 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2509.01055" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://tiger-ai-lab.github.io/verl-tool/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multi-turn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VerlTool, a unified and modular framework that addresses these limitations through systematic design principles. VerlTool provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2Ã— speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multi-turn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">jiang2025verltool</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jiang, Dongfu and Lu, Yi and Li, Zhuofeng and Lyu, Zhiheng and Nie, Ping and Wang, Haozhe and Su, Alex and Chen, Hui and Zou, Kai and Du, Chao and Pang, Tianyu and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{arxiv preprint}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">github</span> <span class="p">=</span> <span class="s">{TIGER-AI-Lab/verl-tool}</span><span class="p">,</span>
  <span class="na">huggingface</span> <span class="p">=</span> <span class="s">{https://huggingface.co/papers/2509.01055}</span><span class="p">,</span>
  <span class="na">twitter</span> <span class="p">=</span> <span class="s">{https://x.com/zhuofengli96475/status/1963216180438814901}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">num_co_first_author</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CIKM 2025</abbr> <figure> <picture> <img src="/assets/img/publication_preview/gref.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="gref.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="lin2025gref" class="col-sm-8"> <div class="title">GReF: A Unified Generative Framework for Efficient Reranking via Ordered Multi-token Prediction</div> <div class="author"> Zhijie Lin, <em>Zhuofeng Li</em>, Chenglei Dai, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Wentian Bao, Shuai Lin, Enyun Yu, Haoxiang Zhang, Liang Zhao' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 34th ACM International Conference on Information and Knowledge Management</em>, Nov 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>In a multi-stage recommendation system, reranking plays a crucial role in modeling intra-list correlations among items. A key challenge lies in exploring optimal sequences within the combinatorial space of permutations. Recent research follows a two-stage (generator-evaluator) paradigm, where a generator produces multiple feasible sequences, and an evaluator selects the best one. In practice, the generator is typically implemented as an autoregressive model. However, these two-stage methods face two main challenges. First, the separation of the generator and evaluator hinders end-to-end training. Second, autoregressive generators suffer from inference efficiency. In this work, we propose a Unified Generative Efficient Reranking Framework (GReF) to address the two primary challenges. Specifically, we introduce Gen-Reranker, an autoregressive generator featuring a bidirectional encoder and a dynamic autoregressive decoder to generate causal reranking sequences. Subsequently, we pre-train Gen-Reranker on the item exposure order for high-quality parameter initialization. To eliminate the need for the evaluator while integrating sequence-level evaluation during training for end-to-end optimization, we propose post-training the model through Rerank-DPO. Moreover, for efficient autoregressive inference, we introduce ordered multi-token prediction (OMTP), which trains Gen-Reranker to simultaneously generate multiple future items while preserving their order, ensuring practical deployment in real-time recommender systems. Extensive offline experiments demonstrate that GReF outperforms state-of-the-art reranking methods while achieving latency that is nearly comparable to non-autoregressive models. Additionally, GReF has also been deployed in a real-world video app Kuaishou with over 300 million daily active users, significantly improving online recommendation quality.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">lin2025gref</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GReF: A Unified Generative Framework for Efficient Reranking via Ordered Multi-token Prediction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Lin, Zhijie and Li, Zhuofeng and Dai, Chenglei and Bao, Wentian and Lin, Shuai and Yu, Enyun and Zhang, Haoxiang and Zhao, Liang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 34th ACM International Conference on Information and Knowledge Management}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">nov</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
  <span class="na">num_co_first_author</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Arxiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/video_eval_pro.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="video_eval_pro.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ma2025videoeval" class="col-sm-8"> <div class="title">VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation</div> <div class="author"> Wentao Ma, Weiming Ren, Yiming Jia, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Zhuofeng Li, Ping Nie, Ge Zhang, Wenhu Chen' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In arxiv preprint</em>, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2505.14640" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://tiger-ai-lab.github.io/VideoEval-Pro/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Large multimodal models (LMMs) have recently emerged as a powerful tool for long video understanding (LVU), but existing benchmarks have significant limitations. Most current benchmarks rely heavily on multiple-choice questions (MCQs) with inflated evaluation results due to guessing, and many questions have strong priors that allow models to answer without watching the entire video. We propose VideoEval-Pro, a new benchmark with open-ended short-answer questions that assess segment-level and full-video understanding through perception and reasoning tasks. By evaluating 21 video LMMs, we discovered significant performance drops (&gt;25\%) on open-ended questions compared to MCQs, higher MCQ scores do not correlate with higher open-ended scores, and VideoEval-Pro benefits more from increased input frames compared to other benchmarks. Our goal is to provide a more realistic and reliable measure of long video understanding.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ma2025videoeval</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{VideoEval-Pro: Robust and Realistic Long Video Understanding Evaluation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ma, Wentao and Ren, Weiming and Jia, Yiming and Li, Zhuofeng and Nie, Ping and Zhang, Ge and Chen, Wenhu}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{arxiv preprint}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">github</span> <span class="p">=</span> <span class="s">{TIGER-AI-Lab/VideoEval-Pro}</span><span class="p">,</span>
  <span class="na">huggingface</span> <span class="p">=</span> <span class="s">{https://huggingface.co/papers/2505.14640}</span><span class="p">,</span>
  <span class="na">twitter</span> <span class="p">=</span> <span class="s">{https://x.com/zhuofengli96475/status/1963216180438814901}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Arxiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/structeval.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="structeval.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="yang2025structeval" class="col-sm-8"> <div class="title">StructEval: Benchmarking LLMsâ€™ Capabilities to Generate Structural Outputs</div> <div class="author"> Jialin Yang, Dongfu Jiang, Lipeng He, and <span class="more-authors" title="click to view 8 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '8 more authors' ? 'Sherman Siu, Yuxuan Zhang, Disen Liao, Zhuofeng Li, Huaye Zeng, Yiming Jia, Haozhe Wang, others' : '8 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">8 more authors</span> </div> <div class="periodical"> <em>In arxiv preprint</em>, May 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2505.20139" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://tiger-ai-lab.github.io/StructEval/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>As Large Language Models (LLMs) become integral to software development workflows, their ability to generate structured outputs has become critically important. We introduce StructEval, a comprehensive benchmark for evaluating LLMsâ€™ capabilities in producing both non-renderable (JSON, YAML, CSV) and renderable (HTML, React, SVG) structured formats. Unlike prior benchmarks, StructEval systematically evaluates structural fidelity across diverse formats through two paradigms: generation tasks and conversion tasks. Our benchmark encompasses 18 formats and 44 types of task, with novel metrics for format adherence and structural correctness. Results reveal significant performance gaps, even state-of-the-art models like o1-mini achieve only 75.58 average score.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">yang2025structeval</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{StructEval: Benchmarking LLMs' Capabilities to Generate Structural Outputs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yang, Jialin and Jiang, Dongfu and He, Lipeng and Siu, Sherman and Zhang, Yuxuan and Liao, Disen and Li, Zhuofeng and Zeng, Huaye and Jia, Yiming and Wang, Haozhe and others}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{arxiv preprint}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">github</span> <span class="p">=</span> <span class="s">{StructEval/StructEval}</span><span class="p">,</span>
  <span class="na">huggingface</span> <span class="p">=</span> <span class="s">{https://huggingface.co/papers/2505.20139}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS 2024</abbr> <figure> <picture> <img src="/assets/img/publication_preview/tegdb.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="tegdb.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2024teg" class="col-sm-8"> <div class="title">Teg-db: A comprehensive dataset and benchmark of textual-edge graphs</div> <div class="author"> <em>Zhuofeng Li</em>, Zixing Gou, Xiangnan Zhang, and <span class="more-authors" title="click to view 6 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '6 more authors' ? 'Zhongyuan Liu, Sirui Li, Yuntong Hu, Chen Ling, Zheng Zhang, Liang Zhao' : '6 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">6 more authors</span> </div> <div class="periodical"> <em>In Advances in Neural Information Processing Systems</em>, Dec 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2406.10310" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/Zhuofeng-Li/TEG-Benchmark" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Text-Attributed Graphs (TAGs) augment graph structures with natural language descriptions, but existing datasets primarily feature textual information only at nodes. We introduce TEG-DB, a comprehensive benchmark of textual-edge datasets with rich descriptions on both nodes and edges, spanning domains like citation and social networks. We conducted benchmark experiments to evaluate how current techniques including pre-trained language models and graph neural networks can utilize textual node and edge information. Our goal is to advance research in textual-edge graph analysis and provide deeper insights into complex real-world networks.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2024teg</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Teg-db: A comprehensive dataset and benchmark of textual-edge graphs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Zhuofeng and Gou, Zixing and Zhang, Xiangnan and Liu, Zhongyuan and Li, Sirui and Hu, Yuntong and Ling, Chen and Zhang, Zheng and Zhao, Liang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Advances in Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{37}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{60980--60998}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">github</span> <span class="p">=</span> <span class="s">{Zhuofeng-Li/TEG-Benchmark}</span><span class="p">,</span>
  <span class="na">huggingface</span> <span class="p">=</span> <span class="s">{https://huggingface.co/datasets/ZhuofengLi/TEG-DB}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Arxiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/link2doc.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="link2doc.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="ling2024link" class="col-sm-8"> <div class="title">Link prediction on textual edge graphs</div> <div class="author"> Chen Ling, <em>Zhuofeng Li</em>, Yuntong Hu, and <span class="more-authors" title="click to view 5 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '5 more authors' ? 'Zheng Zhang, Zhongyuan Liu, Shuang Zheng, Jian Pei, Liang Zhao' : '5 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">5 more authors</span> </div> <div class="periodical"> <em>In arxiv preprint</em>, May 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2405.16606" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Textual-edge Graphs (TEGs) are graphs with rich text annotations on edges, capturing contextual information among entities. Existing approaches often struggle to fully capture edge semantics and graph topology, especially in link prediction tasks. We propose Link2Doc, a novel framework that summarizes neighborhood information between node pairs as a human-written document, preserves both semantic and topological information, and uses a self-supervised learning model to enhance graph neural networksâ€™ text understanding. Empirical evaluations across four real-world datasets demonstrate that Link2Doc achieves generally better performance against existing edge-aware GNNs and pre-trained language models in predicting links on TEGs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ling2024link</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Link prediction on textual edge graphs}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ling, Chen and Li, Zhuofeng and Hu, Yuntong and Zhang, Zheng and Liu, Zhongyuan and Zheng, Shuang and Pei, Jian and Zhao, Liang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{arxiv preprint}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">may</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">num_co_first_author</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">Arxiv</abbr> <figure> <picture> <img src="/assets/img/publication_preview/hireview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="hireview.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="hu2024hireview" class="col-sm-8"> <div class="title">Hireview: Hierarchical taxonomy-driven automatic literature review generation</div> <div class="author"> Yuntong Hu, <em>Zhuofeng Li</em>, Zheng Zhang, and <span class="more-authors" title="click to view 4 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '4 more authors' ? 'Chen Ling, Raasikh Kanjiani, Boxin Zhao, Liang Zhao' : '4 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">4 more authors</span> </div> <div class="periodical"> <em>In arxiv preprint</em>, Jul 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Literature reviews are essential for understanding research fields and identifying knowledge gaps, but manually creating comprehensive reviews is time-consuming and labor-intensive. We present HiReview, a hierarchical taxonomy-driven automatic literature review generation system that leverages large language models to create structured, comprehensive reviews. Our approach uses a multi-level taxonomic framework to organize papers and generate coherent, well-structured literature reviews that capture the essential aspects of a research domain.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">hu2024hireview</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Hireview: Hierarchical taxonomy-driven automatic literature review generation}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Hu, Yuntong and Li, Zhuofeng and Zhang, Zheng and Ling, Chen and Kanjiani, Raasikh and Zhao, Boxin and Zhao, Liang}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{arxiv preprint}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">CIKM 2024</abbr> <figure> <picture> <img src="/assets/img/publication_preview/cfkgc.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cfkgc.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="li2024learning" class="col-sm-8"> <div class="title">Learning from novel knowledge: Continual few-shot knowledge graph completion</div> <div class="author"> <em>Zhuofeng Li</em>, Haoxiang Zhang, Qiannan Zhang, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Ziyi Kou, Shichao Pei' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In Proceedings of the 33rd ACM International Conference on Information and Knowledge Management</em>, Oct 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Knowledge graph completion has been increasingly recognized as a vital approach for uncovering missing knowledge and addressing the incompleteness issue in KGs. To enhance inference on rare relations and mitigate the impact of the long-tail distribution, the dominant strategy designs few-shot models following the meta-learning paradigm. However, these approaches typically operate under the assumption that KGs are available instantly, disregarding the newly emerging relations during KG enrichment. We propose a novel framework designed to equip the few-shot model with the ability to learn sequentially from novel relations through data-level rehearsal and model-level modulation to address catastrophic forgetting, alongside multi-view relation augmentation aimed at resolving the issue of insufficient novel relations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">li2024learning</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning from novel knowledge: Continual few-shot knowledge graph completion}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Li, Zhuofeng and Zhang, Haoxiang and Zhang, Qiannan and Kou, Ziyi and Pei, Shichao}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 33rd ACM International Conference on Information and Knowledge Management}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1326--1335}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">selected</span> <span class="p">=</span> <span class="nv">true</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Zhuofeng Li. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>